### SLAM (Simultaneous Localization And Mapping) 同步定位与建图

让一台设备在完全陌生且没有GPS的环境中，边走边构建地图，同时定位自己在哪里

#### SLAM工作流程：
![alt text](assets/学习/image.png)

#### 三大核心模块：
特称提取与匹配：提取图像关键点（如ORB、SIFT）并进行帧间匹配
位姿估计：利用匹配点计算当前相机相对于上一帧/地图的位姿
地图优化：通过Bundle Adjustment或因子图优化，提高地图一致性

#### SLAM的输入与输出
##### 输入
图像流：单目、双目或RGB-D图像序列
传感器数据：可选，如IMU、激光雷达、GPS
相机内参：焦距、主点等

##### 输出
相机位姿（轨迹）：表示机器人/相机在空间中的轨迹
地图（稀疏/稠密）：表示周围环境中的特征点/障碍物
回环检测信息：用于修正漂移、闭环优化

#### SLAM的分类
视觉SLAM：输入是相机图像，如ORB-SLAM、DSO
惯性SLAM：加入IMU传感器，如VINS-Fusion
激光SLAM：输入是激光雷达点云，如LOAM、Cartographer
多模态SLAM：相机+雷达+IMU，增强鲁棒性LVI-SLAM、FAST-LIO

### UAV (Unmanned Aerial Vehicle) 无人机

### VLN (Visual-Language Navigation) 视觉语言导航

#### 视觉语言导航常见任务
R2R(Room-to-Room)
智能体基于一段简短的英文指令，从一个房间移动到另一个房间

RxR(Room-Across-Room)
时空对齐：指令中的词语和智能体在路径上看到的视觉画面在时间轴上也是对齐的
路径比R2R更复杂，指令描述也更详尽

REVERIE(Remote Embodied Visual Referencing Expression)
智能体需要根据一句高层指令，导航到目标位置并识别出特定物体

TouchDown
VLN任务从室内到室外场景的延伸

ALFRED(Action Learning From Realistic Environments and Directives)
智能体不仅要导航，还要执行拾取、放置等物理动作

iGibson
高保真模拟环境系统，不仅提供导航服务
支持机器人与环境中数千种物体的物理交互，包含多层房屋的大型室内场景

#### VLN基准模型
Seq2Seq模型
是在R2R论文中使用的第一个基准模型
将自然语言指令翻译成一系列动作序列
编码器(Encoder)：通常使用一个循环神经网络（如LSTM或GRU）将输入的文本指令编码为一个固定长度的向量
解码器(Decoder)：在每个时间步t，解码器根据当前的指令向量和当前观察到的视觉图像，预测下一步动作At
局限性：传统的Seq2Seq模型很难在长指令中实时对应当前走到了哪一步，容易在长路径中迷失

CMA(Cross-Modal Attention)模型
在Seq2Seq基础上引入更精细的跨模态注意力机制
在每一个步骤中，智能体不仅看一眼指令，还会计算当前视觉特征与指令中每个词的相关性
例如，如果智能体现在看到了“洗手池”，注意力机制会使其重点关注指令中“Bathroom”或“Sink”这些词。